{
  "id": 12,
  "title": "Module 2 Assessment",
  "description": "Test your understanding of high-risk AI compliance requirements.",
  "type": "quiz",
  "duration": 20,
  "questions": [
    {
      "id": 1,
      "question": "Which article establishes risk management requirements for high-risk AI?",
      "type": "mcq",
      "options": [
        "Article 6",
        "Article 9",
        "Article 12",
        "Article 15"
      ],
      "correctAnswer": 1,
      "explanation": "Article 9 establishes the requirements for a continuous, iterative risk management system throughout the AI system lifecycle."
    },
    {
      "id": 2,
      "question": "When must technical documentation be prepared?",
      "type": "mcq",
      "options": [
        "Within 30 days of deployment",
        "Before placing on market",
        "Upon authority request",
        "After first incident"
      ],
      "correctAnswer": 1,
      "explanation": "Article 11(1) requires technical documentation to be drawn up before the high-risk AI system is placed on the market or put into service."
    },
    {
      "id": 3,
      "question": "How long must providers retain technical documentation?",
      "type": "mcq",
      "options": [
        "5 years",
        "7 years",
        "10 years",
        "15 years"
      ],
      "correctAnswer": 2,
      "explanation": "Article 18 requires providers to keep technical documentation for 10 years after the AI system has been placed on the market."
    },
    {
      "id": 4,
      "question": "What is the minimum log retention period for deployers?",
      "type": "mcq",
      "options": [
        "30 days",
        "3 months",
        "6 months",
        "1 year"
      ],
      "correctAnswer": 2,
      "explanation": "Article 26(6) requires deployers to keep logs generated by high-risk AI systems for a period appropriate to the intended purpose, with a minimum of 6 months."
    },
    {
      "id": 5,
      "question": "Which deployers must conduct fundamental rights impact assessments?",
      "type": "mcq",
      "options": [
        "All deployers",
        "Only providers",
        "Public bodies and entities providing public services",
        "Only large enterprises"
      ],
      "correctAnswer": 2,
      "explanation": "Article 27 requires fundamental rights impact assessments by public bodies and entities providing public services, and private entities where AI decisions affect fundamental rights."
    },
    {
      "id": 6,
      "question": "What must human oversight enable?",
      "type": "mcq",
      "options": [
        "Only monitoring",
        "Intervention and stop capability",
        "Only logging",
        "Only reporting"
      ],
      "correctAnswer": 1,
      "explanation": "Article 14 requires human oversight to include the ability to intervene in operation and interrupt through a 'stop' button or similar procedure."
    },
    {
      "id": 7,
      "question": "Which conformity assessment route applies to most Annex III high-risk AI?",
      "type": "mcq",
      "options": [
        "Notified body only",
        "Internal control",
        "Self-certification only",
        "No assessment required"
      ],
      "correctAnswer": 1,
      "explanation": "Most Annex III high-risk AI systems can use internal control procedures (Annex VI), though biometric identification systems often require notified body assessment."
    },
    {
      "id": 8,
      "question": "What triggers the need to repeat conformity assessment?",
      "type": "mcq",
      "options": [
        "Annual requirement",
        "Substantial modification",
        "Customer request",
        "Never required to repeat"
      ],
      "correctAnswer": 1,
      "explanation": "Conformity assessment must be repeated if there are substantial modifications to the AI system that affect its compliance status."
    },
    {
      "id": 9,
      "question": "Which cybersecurity threat is NOT specifically mentioned in Article 15?",
      "type": "mcq",
      "options": [
        "Data poisoning",
        "Adversarial examples",
        "SQL injection",
        "Model poisoning"
      ],
      "correctAnswer": 2,
      "explanation": "Article 15 specifically mentions AI-specific threats: data poisoning, model poisoning, adversarial examples, and confidentiality attacks. SQL injection is a general cybersecurity issue."
    },
    {
      "id": 10,
      "question": "What must be declared in instructions for use regarding accuracy?",
      "type": "mcq",
      "options": [
        "Only general statements",
        "Accuracy levels and relevant metrics",
        "No disclosure required",
        "Only worst-case scenarios"
      ],
      "correctAnswer": 1,
      "explanation": "Article 15(2) requires that accuracy levels and relevant accuracy metrics be declared in the instructions for use accompanying the AI system."
    }
  ]
}