# Deployer Obligations (Article 26)

## Learning Objectives

By the end of this chapter, you will be able to:
- Understand the complete scope of deployer obligations under Article 26
- Implement effective human oversight programmes for AI systems
- Conduct fundamental rights impact assessments (FRIA)
- Establish compliant log retention and monitoring practices
- Navigate GDPR integration requirements for AI deployment
- Manage worker information and consultation requirements

---

Deployers of high-risk AI systems have specific obligations under Article 26. While less extensive than provider obligations, these requirements are essential for ensuring high-risk AI is used safely, ethically, and in compliance with fundamental rights.

## Understanding the Deployer Role

### Who is a Deployer?

**Article 3(4) Definition:** A deployer is any natural or legal person, public authority, agency, or other body using an AI system under its authority, **except where the AI system is used in the course of a personal non-professional activity**.

### Provider vs. Deployer Distinction

| Criterion | Provider | Deployer |
| --- | --- | --- |
| **Core activity** | Develops/places on market | Uses under own authority |
| **Compliance burden** | Extensive (Articles 8-22) | Focused (Article 26) |
| **Pre-market obligations** | Yes | No |
| **Post-market obligations** | Yes | Limited |
| **Typical examples** | AI vendors, software companies | Enterprises, public authorities |

### When Deployers Become Providers

**Article 25** triggers provider status when a deployer:

| Action | Result |
| --- | --- |
| Places their name/trademark on high-risk AI | Becomes provider for that system |
| Makes substantial modification | Becomes provider for modified system |
| Changes intended purpose to high-risk | Becomes provider for new purpose |

> âš ï¸ **Critical Warning:** Modifications to AI systemsâ€”even seemingly minor changesâ€”may trigger provider obligations. Always assess modifications against Article 25 criteria before implementation.

## Core Deployer Obligations (Article 26(1)-(7))

### The Complete Obligation Framework

| Obligation | Article Reference | Key Requirements |
| --- | --- | --- |
| **Use per instructions** | Article 26(1) | Follow provider's instructions for use |
| **Human oversight** | Article 26(2) | Assign competent persons with authority |
| **Input data relevance** | Article 26(4) | Ensure data relevant to intended purpose |
| **Operation monitoring** | Article 26(5) | Monitor per instructions, report incidents |
| **Log retention** | Article 26(6) | Keep logs minimum 6 months |
| **Worker information** | Article 26(7) | Inform workers before AI implementation |
| **FRIA (if applicable)** | Article 27 | Conduct fundamental rights impact assessment |
| **Data protection** | Article 26(9)-(10) | Comply with GDPR, conduct DPIA if required |
| **Authority cooperation** | Article 26(11) | Provide information, access, cooperation |

## Using AI Systems According to Instructions

### Instruction Compliance Requirements

**Article 26(1)** requires deployers to take appropriate technical and organisational measures to ensure they use high-risk AI systems in accordance with the instructions for use.

| Instruction Element | Deployer Action |
| --- | --- |
| **Intended purpose** | Verify deployment matches stated purpose |
| **Operating environment** | Ensure environment meets specifications |
| **Input requirements** | Provide data meeting quality criteria |
| **User competency** | Train staff per provider requirements |
| **Limitations** | Respect stated system limitations |
| **Maintenance** | Follow maintenance and update procedures |

### Documenting Instruction Compliance

Maintain records demonstrating:

- [ ] Instructions for use received and reviewed
- [ ] Deployment context aligns with intended purpose
- [ ] Operating environment meets specifications
- [ ] Staff trained on system use
- [ ] Limitations understood and communicated
- [ ] Deviations from instructions (if any) documented and justified

> ðŸ’¡ **Best Practice:** Create a deployment checklist based on the instructions for use. Complete this checklist before putting any high-risk AI system into service and retain it as compliance evidence.

## Human Oversight Implementation

### Article 26(2) Requirements

Deployers must assign human oversight to natural persons who have:

| Requirement | Description | Evidence |
| --- | --- | --- |
| **Competence** | Necessary skills and knowledge | Training records, qualifications |
| **Training** | Specific training on the AI system | Completion certificates, assessment results |
| **Authority** | Power to override or halt AI system | Role descriptions, delegation records |

### Human Oversight Framework

**Oversight Roles and Responsibilities:**

| Role | Responsibilities | Authority Level |
| --- | --- | --- |
| **Day-to-day Operator** | Monitor outputs, flag anomalies | Escalate concerns |
| **System Supervisor** | Review flagged cases, approve high-stakes decisions | Override specific outputs |
| **AI Governance Lead** | Oversee compliance, manage incidents | Suspend system operation |
| **Executive Sponsor** | Strategic accountability, resource allocation | Terminate deployment |

### Addressing Automation Bias (Article 26(2))

**Article 26(2)** explicitly requires deployers to ensure oversight persons:

- Are aware of the possible tendency to over-rely on AI outputs ("automation bias")
- Are able to correctly interpret outputs in context
- Are able to decide not to use the AI system or disregard its output

**Automation Bias Mitigation Strategies:**

| Strategy | Implementation |
| --- | --- |
| **Training** | Include automation bias awareness in all AI training |
| **Dual review** | Require independent human review for high-stakes decisions |
| **Confidence display** | Show AI confidence levels to users |
| **Explanation provision** | Require AI to explain reasoning where possible |
| **Regular calibration** | Compare AI outputs against ground truth |
| **Override tracking** | Monitor when and why humans override AI |

### Human Oversight Documentation

Maintain records of:

- [ ] Oversight personnel assignments
- [ ] Competency assessments
- [ ] Training completion records
- [ ] Override events and rationale
- [ ] Escalation procedures followed
- [ ] Regular performance reviews

## Input Data Quality (Article 26(4))

### Ensuring Input Data Relevance

Deployers must ensure input data is **relevant and sufficiently representative** in view of the intended purpose of the high-risk AI system.

| Data Quality Dimension | Deployer Responsibility |
| --- | --- |
| **Relevance** | Input data matches the system's intended use case |
| **Representativeness** | Data reflects the population/scenarios where AI is applied |
| **Completeness** | Required data fields are populated |
| **Accuracy** | Data is correct and current |
| **Timeliness** | Data is sufficiently recent for the use case |

### Data Quality Monitoring

Implement ongoing data quality checks:

- [ ] Define data quality criteria aligned with instructions for use
- [ ] Establish data validation at point of entry
- [ ] Monitor for data drift or distribution changes
- [ ] Report data quality issues to provider if affecting system performance
- [ ] Document data quality assessments and actions taken

## Operation Monitoring (Article 26(5))

### Monitoring Requirements

Deployers must monitor high-risk AI operation on the basis of the instructions for use and, where relevant, inform providers in accordance with Article 72(1).

**Monitoring Framework:**

| Monitoring Aspect | Frequency | Action Triggers |
| --- | --- | --- |
| **Performance accuracy** | Continuous/periodic | Deviation beyond threshold |
| **Output quality** | Per use | Anomalous outputs |
| **User feedback** | Ongoing | Complaints or concerns |
| **Incident detection** | Continuous | Any malfunction or harm |
| **Compliance status** | Periodic | Audit findings |

### Incident Reporting to Providers

When monitoring reveals issues, deployers must inform providers if:

- Performance degrades significantly
- Unexpected outputs or behaviours occur
- Users report problems or concerns
- Incidents cause or risk harm

## Log Retention (Article 26(6))

### Minimum Retention Requirements

| Log Type | Minimum Period | Notes |
| --- | --- | --- |
| **Automatically generated logs** | 6 months minimum | Or longer if required by other law |
| **Appropriate to intended purpose** | As specified | Provider may specify longer periods |
| **Under deployer control** | Throughout | Deployer responsible for retention |

### Log Management Best Practices

- [ ] Establish log retention policy aligned with AI Act and other regulations
- [ ] Implement secure, tamper-evident log storage
- [ ] Create log retrieval procedures for authority requests
- [ ] Document log contents and format
- [ ] Plan for log handover if AI system is transferred
- [ ] Consider longer retention where fundamental rights implicated

> âš ï¸ **Important:** GDPR may require data minimisation while AI Act requires log retention. Resolve this tension by ensuring logs contain minimum necessary personal data and implementing appropriate access controls.

## Worker Information Requirements (Article 26(7))

### Mandatory Information Provision

**Before** putting a high-risk AI system into use at the workplace, deployers must inform:

- Workers
- Worker representatives (if any)

### Information Content

| Information Element | Detail Required |
| --- | --- |
| **AI system deployment** | What AI system, when deployed |
| **Intended purpose** | How AI will be used in workplace |
| **Impact on workers** | How workers may be affected |
| **Decision scope** | What decisions AI will inform/make |
| **Human oversight** | Who oversees AI, how to raise concerns |
| **Rights** | Workers' rights regarding AI decisions |

### Works Council and Union Consultation

Where applicable, integrate AI deployment into existing consultation frameworks:

- Inform works councils in advance of deployment
- Consult on impact assessments
- Address concerns raised by worker representatives
- Update collective agreements if needed

## Fundamental Rights Impact Assessment (Article 27)

### When FRIA is Required

**Article 27** mandates FRIA for deployers that are:

| Deployer Type | Requirement |
| --- | --- |
| **Bodies governed by public law** | Mandatory FRIA |
| **Private entities providing public services** | Mandatory FRIA |
| **Credit institutions (credit scoring)** | Mandatory FRIA |
| **Life/health insurance (risk assessment)** | Mandatory FRIA |
| **Other private entities** | Where legally required or best practice |

### FRIA Content Requirements

The impact assessment must contain:

| Element | Description |
| --- | --- |
| **Deployer's processes** | Description of processes where AI will be used |
| **Frequency and categories** | How often used, categories of persons affected |
| **Specific risks** | Identified risks to fundamental rights of affected persons |
| **Human oversight measures** | Measures to implement human oversight |
| **Risk mitigation measures** | Specific measures to address identified risks |
| **Governance measures** | Organisational oversight arrangements |
| **Complaints mechanism** | Process for affected persons to raise concerns |

### FRIA Process Framework

**Phase 1: Scoping**
- Identify AI system and intended deployment
- Determine affected fundamental rights
- Identify categories of affected persons

**Phase 2: Risk Assessment**
- Analyse potential impacts on each affected right
- Consider direct and indirect effects
- Assess likelihood and severity of harms

**Phase 3: Mitigation Design**
- Develop measures to address identified risks
- Design human oversight implementation
- Create complaints and redress mechanisms

**Phase 4: Documentation and Notification**
- Document complete assessment
- Notify market surveillance authority (public sector)
- Review and update periodically

> ðŸ’¡ **Expert Tip:** Align FRIA with existing DPIA processes. Many fundamental rights impacts overlap with data protection impacts, and integrated assessments are more efficient and comprehensive.

## GDPR Integration (Article 26(9)-(10))

### DPIA Requirements

**Article 26(9)** requires deployers to use provider-supplied information to conduct Data Protection Impact Assessments (DPIA) where required by GDPR Article 35.

**DPIA Triggers for AI:**

| Trigger | AI Context |
| --- | --- |
| **Systematic profiling** | AI-based profiling affecting individuals |
| **Large-scale sensitive data** | AI processing biometric, health data |
| **Systematic monitoring** | AI surveillance of public spaces |
| **New technologies** | Novel AI applications with uncertain impacts |

### GDPR Controller Responsibilities

Where deploying AI that processes personal data:

- [ ] Identify legal basis for processing
- [ ] Implement data minimisation
- [ ] Ensure data subject rights can be exercised
- [ ] Conduct DPIA where required
- [ ] Document processing activities
- [ ] Implement appropriate security measures

## Authority Cooperation (Article 26(11))

### Cooperation Obligations

Deployers must cooperate with market surveillance authorities including:

| Requirement | Deployer Action |
| --- | --- |
| **Information provision** | Provide requested information within timeframes |
| **Access** | Grant access to AI system, logs, documentation |
| **Testing support** | Facilitate technical testing if required |
| **Corrective action** | Implement required corrective measures |

### Preparing for Authority Requests

Maintain readiness by:

- [ ] Designating authority liaison contact
- [ ] Establishing document retrieval procedures
- [ ] Testing access provision capabilities
- [ ] Training relevant staff on cooperation requirements

## Compliance Checklist: Deployer Obligations

**Pre-Deployment:**
- [ ] Review instructions for use thoroughly
- [ ] Verify deployment aligns with intended purpose
- [ ] Assign human oversight personnel
- [ ] Train staff on AI system use
- [ ] Conduct FRIA (if required)
- [ ] Conduct DPIA (if required)
- [ ] Inform workers and representatives

**Operational:**
- [ ] Monitor AI performance per instructions
- [ ] Maintain log retention systems
- [ ] Track human oversight activities
- [ ] Address data quality issues
- [ ] Report incidents to providers

**Ongoing:**
- [ ] Review and update FRIA/DPIA periodically
- [ ] Refresh staff training
- [ ] Assess for substantial modifications
- [ ] Maintain authority cooperation readiness

## Key Takeaways

- Deployers must use high-risk AI strictly according to provider instructions for use
- Human oversight requires persons with competence, training, and authority to override AI
- Automation bias awareness is an explicit requirementâ€”train staff to recognise over-reliance risks
- Log retention minimum is 6 months; longer may be required for other regulatory purposes
- Workers must be informed before AI deployment in the workplace
- Public sector and certain private deployers must conduct fundamental rights impact assessments
- GDPR compliance runs parallelâ€”DPIA may be required alongside AI Act obligations
- Cooperation with market surveillance authorities is mandatory