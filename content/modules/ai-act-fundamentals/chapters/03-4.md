# Risk Classification Framework

## Learning Objectives

By the end of this chapter, you will be able to:
- Apply the AI Act's risk classification methodology to any AI system
- Distinguish between the two pathways to high-risk classification (Article 6)
- Navigate Annex I (product safety) and Annex III (standalone) high-risk categories
- Understand the "filter" exception that can downgrade Annex III systems
- Conduct a preliminary risk classification assessment

---

The AI Act's **risk-based approach** is its regulatory cornerstone. Classification determines whether your AI system is prohibited, subject to extensive requirements, requires transparency only, or faces no mandatory obligations. This chapter provides a systematic methodology for classification.

## The Risk Classification Methodology

Classification follows a **sequential assessment** through four risk tiers:

[diagram:risk-classification-flow]

### Step 1: Is it Prohibited? (Article 5)
First, determine if the AI practice falls under the eight prohibited categories. If yes, the AI cannot be legally deployed in the EU‚Äîstop here.

### Step 2: Is it High-Risk? (Article 6)
If not prohibited, assess whether the AI meets either high-risk pathway:
- **Pathway A:** Safety component in Annex I product requiring third-party conformity assessment
- **Pathway B:** Standalone AI in an Annex III use case (subject to "filter" exception)

### Step 3: Does it Require Transparency? (Article 50)
If not high-risk, check transparency triggers:
- Direct human interaction (chatbots)
- Synthetic content generation (deepfakes)
- Emotion recognition or biometric categorisation (permitted contexts)

### Step 4: Minimal Risk
If none of the above apply, the system is minimal risk with no mandatory requirements.

[diagram:classification-decision-flow]

## High-Risk Pathway A: Annex I Product Safety (Article 6(1))

AI systems are high-risk under **Article 6(1)** if they meet BOTH conditions:

1. **The AI is a safety component** of a product, or is itself a product, covered by EU harmonisation legislation listed in Annex I
2. **The product requires third-party conformity assessment** under that legislation

### Annex I Sectors (Product Safety Legislation)

| Sector | EU Legislation | Example AI Applications |
| --- | --- | --- |
| **Machinery** | Regulation (EU) 2023/1230 | Industrial robots, autonomous vehicles |
| **Toys** | Directive 2009/48/EC | AI-enabled interactive toys |
| **Recreational Craft** | Directive 2013/53/EU | Autonomous navigation systems |
| **Lifts** | Directive 2014/33/EU | AI-controlled elevator systems |
| **Equipment in Explosive Atmospheres** | Directive 2014/34/EU | Predictive maintenance AI |
| **Radio Equipment** | Directive 2014/53/EU | AI in wireless devices |
| **Pressure Equipment** | Directive 2014/68/EU | AI monitoring systems |
| **Cableways** | Regulation (EU) 2016/424 | Autonomous operation AI |
| **Personal Protective Equipment** | Regulation (EU) 2016/425 | Smart safety equipment |
| **Gas Appliances** | Regulation (EU) 2016/426 | AI combustion control |
| **Medical Devices** | Regulation (EU) 2017/745 | AI diagnostic software |
| **In-Vitro Diagnostics** | Regulation (EU) 2017/746 | AI analysis systems |
| **Civil Aviation** | Regulation (EU) 2018/1139 | Autopilot, air traffic AI |
| **Motor Vehicles** | Regulation (EU) 2019/2144 | ADAS, autonomous driving |
| **Agricultural Vehicles** | Regulation (EU) 167/2013 | Autonomous tractors |
| **Rail Systems** | Directive (EU) 2016/797 | Train control AI |

> üí° **Expert Insight:** The Annex I pathway primarily captures AI embedded in physical products already subject to EU safety regulation. The AI Act adds AI-specific requirements on top of existing product safety obligations.

## High-Risk Pathway B: Annex III Use Cases (Article 6(2))

AI systems are high-risk under **Article 6(2)** if they fall within the use cases enumerated in **Annex III**‚Äîregardless of the product in which they are deployed.

### Annex III: The Eight High-Risk Domains

**Section 1: Biometrics (where permitted)**
- Remote biometric identification systems
- Biometric categorisation systems
- Emotion recognition systems (non-prohibited contexts)

**Section 2: Critical Infrastructure**
- AI safety components in management/operation of:
  - Road traffic
  - Water, gas, heating, electricity supply
  - Digital infrastructure

**Section 3: Education and Vocational Training**
- AI determining access to education institutions
- AI evaluating learning outcomes
- AI assessing appropriate education level
- AI monitoring prohibited behaviour during tests

**Section 4: Employment, Workers Management, Self-Employment Access**
- Recruitment and selection (CV screening, interviews)
- Promotion, termination, task allocation decisions
- Performance and behaviour monitoring

**Section 5: Access to Essential Services**
- Credit worthiness evaluation (individuals)
- Life and health insurance risk assessment (individuals)
- Emergency services dispatch prioritisation

**Section 6: Law Enforcement (where permitted)**
- Individual risk assessment (criminal offending)
- Polygraphs and similar tools
- Evidence reliability assessment
- Offence risk assessment (profiling exception)
- Crime analytics for patterns

**Section 7: Migration, Asylum, Border Control**
- Polygraphs and similar tools
- Immigration/asylum/visa application risk assessment
- Document authenticity verification
- Visa/permit/complaint examination assistance

**Section 8: Administration of Justice and Democracy**
- Judicial fact and law research assistance
- AI with potential to influence electoral outcomes

### The "Filter" Exception (Article 6(3))

A critical nuance: **Annex III AI systems are NOT automatically high-risk**. Article 6(3) provides a "filter" exception:

An Annex III AI system is **not high-risk** if it does not pose a significant risk of harm to health, safety, or fundamental rights, taking into account:

| Filter Criterion | Meaning |
| --- | --- |
| **Narrow procedural task** | Performs very specific, limited function |
| **Improves prior human activity** | Enhances rather than replaces human decision |
| **Preparatory nature** | Prepares for human decision, doesn't make it |
| **Low deviation impact** | Human can easily deviate from AI recommendation |
| **No meaningful impact** | Output doesn't significantly affect outcomes |

> ‚ö†Ô∏è **Compliance Warning:** The filter exception is **narrow and must be justified**. Providers must document their filter analysis. If uncertain, treat the system as high-risk.

**Filter Exception Does NOT Apply To:**
- Biometric identification and categorisation (always high-risk if in Annex III)
- AI used for profiling of natural persons

## Limited Risk: Transparency Obligations (Article 50)

AI systems that are not prohibited or high-risk may still have **transparency obligations**:

| Trigger | Obligation |
| --- | --- |
| **AI interacting directly with humans** | Inform that interacting with AI (unless obvious) |
| **Emotion recognition or biometric categorisation** | Inform affected persons, GDPR applies |
| **Synthetic content generation** (deepfakes) | Disclose AI-generated content |
| **Text published for public information** | Disclose AI generation (exceptions for editorial process) |

## Minimal Risk: Voluntary Measures

The majority of AI systems fall into **minimal risk** with no mandatory requirements:
- Spam filters
- AI-enabled video games
- Inventory management AI
- General recommendation systems

Providers may **voluntarily** adopt codes of conduct (Article 95) applying high-risk requirements.

## Risk Classification Quick Assessment

**Use this checklist for preliminary classification:**

| Question | Yes | No |
| --- | --- | --- |
| Is this a prohibited practice under Art. 5? | **STOP - Prohibited** | Continue |
| Is this AI a safety component in an Annex I product requiring 3rd party assessment? | **High-Risk (Pathway A)** | Continue |
| Is this AI in an Annex III use case? | Check filter exception | Continue |
| Does the filter exception apply AND AI is not biometric/profiling? | Limited/Minimal Risk | **High-Risk (Pathway B)** |
| Does Art. 50 transparency apply? | **Limited Risk** | **Minimal Risk** |

## Key Takeaways

- The AI Act uses a **four-tier risk classification**: prohibited, high-risk, limited risk, minimal risk
- **Two pathways** lead to high-risk: Annex I (product safety) and Annex III (use cases)
- Annex III systems may escape high-risk through the **"filter" exception** if they pose no significant risk
- The filter **does not apply** to biometric and profiling AI‚Äîthese remain high-risk
- **Transparency obligations** (Article 50) apply to direct interaction, emotion recognition, and synthetic content
- **Most AI systems** are minimal risk with no mandatory requirements
- When uncertain, **classify conservatively** as high-risk and document your reasoning